爬蟲: 讓程式幫你上網

# 爬蟲 SOP
1. 打開原始碼(網址列的回覆)
2. 需要的東西
a. in 原始碼: 網址列
b. not in 原始碼: F12 -> 在Network裡找
JSON 格式 -> list + dict

# 路徑
1. 絕對路徑 -> C://xxxx/xxx/xxx
2. 相對路徑(相對目前檔案的位置)
a. 跟目前檔案同一個資料夾 -> xx.txt or ./xx.txt
b. 在目前檔案的前一層資料夾
../c.txt
c. 在目前檔案的前兩層資料夾
../../d.txt

a(dir) -- b(dir)     ---- main.py
d.txt  -- c.txt      ---- xx.txt

# pandas函式庫
CSV/TSV(Comma-Separated Values)/(TAB...)
姓名, 身高\n
Elwing, 175\n
"Elwing, Mr. Chou", 180

# 例題練習
#1 google 好手氣(下載裡面的圖片)
Python內建資料庫 -> urllib(資料夾) -> request.py
a. urlopen(網址): 送出網址得到回覆
b. urlretrieve(網址, 指定下載位址): 送出網址得到回覆，回覆下載到指定位址

import os  # 所有跟作業系統有關的函示庫
import json
import urllib.request as req  # 給函式庫一個簡短的寫法

dn = "google"
if not os.path.exists(dn):  # 如果這個資料夾不存在
    os.makedirs(dn)  # 那就建立一個資料夾

url = "https://www.google.com/doodles/json/2022/5?hl=zh-TW"
response = req.urlopen(url)
# 另一種寫法: pics = json.load(response)
# 回應的是一種檔案類型: .read()
r = response.read()
pics = json.loads(r)

for p in pics:  # p的型態是字典
    print(p["title"])
    imgurl = "https:" + p["high_res_url"]
    print(imgurl)
    # urlretrieve(網址, 下載路徑)
    fn = imgurl.split("/")[-1]
    totalfn = dn + "/" + fn
    req.urlretrieve(imgurl, totalfn)
    print("-" * 30)

#2 Youtube(不會爬蟲)
1. 找到隱藏網址
f12 -> network -> 上方選擇Fetch/XHR(youtube上是串流影片，影片被分割成一塊一塊，不能算是完整的media) -> 找到第一個名稱開頭是video的(videoplayback)

1.1 確認正確檔名
Response Headers -> content-type: video/mp4

2. 下載完整影片
複製General裡第一個網址 -> 找到xxxx?參數=值&參數=值 -> 其中一段是&range=0-xxxx -> 刪除

3. 下載完整聲音
找到第二個名稱開頭是video的 -> 複製General裡第一個網址 -> 找到xxxx?參數=值&參數=值 -> 其中一段是&range=0-xxxx -> 刪除

3.1 確認正確檔名
Response Headers -> content-type: audio/webm

4. 轉檔 / 合併
Windows:
https://drive.google.com/file/d/1NG1jLgtNSMA8Gu_WKtfpa7a7sOsXoRiI/view?usp=sharing
有圖形化介面，操作簡單
ffmpeg / ffplay / ffprobe
轉檔(下載的聲音檔轉成mp3): FFQueue -> add -> ... -> 選擇自己下載的檔案 -> Outputfile改副檔名 -> savejob -> start
合併(將畫面及聲音檔合併):  addinputfile -> ... -> 選擇自己下載的檔案 -> 匯入的檔案每個都勾 -> Outputfile改副檔名 -> savejob -> start

MAC:
https://drive.google.com/file/d/1jeMX_L6j4iINj9f9pRXyddxeHPuLkQJu/view?usp=sharing
優點: 免費影片編輯的工具，由全世界工程師維護，可以大量上字幕 / 浮水印 / 合併影片
缺點: 沒有圖形化介面
轉檔: 終端機 -> 將目的地資料夾拖曳到終端機 -> 將ffmpeg拖曳到終端機 -> 按空格再加上-i -> 按空格再加上聲音檔(拖曳到終端機) -> 按空格再加上檔名.mp3
合併: ffmpeg -i 聲音 -i 影片 out.mp4

執行檔案 參數-參數名稱 參數值
cd 到
~ 根目錄

#3 巴哈姆特動畫瘋(不算爬蟲)
1. 找到隱藏網址
f12 -> network -> 上方選擇Fetch/XHR -> 找到chunklist_b400000(播放清單)
danmuGet.php(彈幕)

2. 下載完整影片
複製General裡第一個網址 -> 打開終端機
Windows: -headers "Origin: https://ani.gamer.com.tw" -i "" output.mp4(自己設定檔名) -> 引號內加入影片網址
MAC: -headers 'Origin: https://ani.gamer.com.tw' -i '' output.mp4(自己設定檔名) -> 引號內加入影片網址

#4 tabelog 以此網址為例: https://tabelog.com/tw/tokyo/rstLst/?SrtT=rt
請列印出商家中文名稱、日文名稱、菜的類別、評分、午餐價格、晚餐價格、網址
# from 基準點 import .py
from urllib.request import urlopen  # 只import需要用到的功能 
# 可讀性比較好
from bs4 import BeautifulSoup
url = "https://tabelog.com/tw/tokyo/rstLst/?SrtT=rt"
response = urlopen(url)
html = BeautifulSoup(response)  # 型態轉換 # 出現紅字不用緊張只是warning不是error
# 區塊.find/find_all(區塊html名字, 篩選條件class/id)
rs = html.find_all("li", {"class":"list-rst"})
# rs: [li, li, li, ...]
for r in rs:
    # r: 一個li區塊
    en = r.find("a", {"class":"list-rst__name-main"})
    ja = r.find("small", {"class":"list-rst__name-ja"})
    rtype = r.find("li", {"class":"list-rst__catg"})
    rating = r.find("b", {"class":"c-rating__val"})
    price = r.find_all("span", {"class":"c-rating__val"})
    # 型態擁有兩種東西: 1. 專屬功能 2. 專屬值 型態.值
    # html內容(專屬值的拿法): 區塊.text
    # html屬性(href): 區塊["href"]
    print(en.text)  # 中文名字
    print(ja.text)  # 日文名字
    print(rtype.text)  # 菜的類別
    print(rating.text)  # 評分
    print("午餐:", price[0].text)  # 午餐價格
    print("晚餐:", price[1].text)  # 晚餐價格
    print(en["href"])  # 網址
    print("-" * 30)
    
#5 PTT(https://www.ptt.cc/bbs/Beauty/M.1657591280.A.355.html)
1. 確認是否需要找隱藏網址: 右鍵檢查原始碼 -> 裡面已經有需要的圖片網址，就不需要找隱藏網址

2. 出現紅字
如果出現 HTTP Error: 403 有兩種可能
a. IP被封鎖
b. 送出的請求跟瀏覽器不一樣(Headers)
from urllib.request import urlopen

url = "https://www.ptt.cc/bbs/Beauty/M.1657591280.A.355.html"
response = urlopen(url)
從Headers -> Request Headers -> referer可以得知你是從哪個網站過來的
從Headers -> Request Headers -> user-agent可以知道你的瀏覽器版本

3. 送出完整的請求(網址、Headers)
from urllib.request import urlopen, Request
from bs4 import BeautifulSoup

url = "https://www.ptt.cc/bbs/Beauty/M.1657591280.A.355.html"
req = Request(url)
req.add_header("user-agent", "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36")
response = urlopen(req)
html = BeautifulSoup(response)
print(html)
會發現不是顯示預期的網站，而是問你是否已滿18歲

4. 把cookie帶進去(已滿18)
網址送出去給伺服器時在General顯示Status Code: 302(轉址)，被轉到問你是否滿18歲的網址，按下已滿18會把https://www.ptt.cc/ask/over18送出，再被轉址成正確網址
F12 -> over18 -> Response Headers會多一個set-cookie: over18=1; Path=/，代表之後再進去這個網址就不會被問問題，瀏覽器已經記住這個cookie了
*cookie: 當你做過一次操作後，某些值會被存在瀏覽器，再拜訪網站時這些值會被自動送出去(像是記住帳號密碼、自動登入)
from urllib.request import urlopen, Request
from bs4 import BeautifulSoup

url = "https://www.ptt.cc/bbs/Beauty/M.1657591280.A.355.html"
req = Request(url)
req.add_header("user-agent", "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36")
# 有些網站不會檢查這麼嚴，可以打 "Mozilla/5.0" 就好
req.add_header("cookie", "over18=1")
response = urlopen(req)
html = BeautifulSoup(response)
print(html)

5. 改import requests函式庫()
a. 使用方式(API)比較好用
b. 已經幫你加上基本Headers -> user-agent: Mozilla 5.0(如果遇到403你就要加滿)
一開始不教是因為如果遇到HTTP error: 403會不知道是Headers的問題
import requests
from bs4 import BeautifulSoup

url = "https://www.ptt.cc/bbs/Beauty/M.1657591280.A.355.html"
response = requests.get(url)  # 代替urlopen，也不能直接丟到BeautifulSoup
html = BeautifulSoup(response.text)
print(html)

6. 帶入cookie(已滿18)
import requests
from bs4 import BeautifulSoup

url = "https://www.ptt.cc/bbs/Beauty/M.1657591280.A.355.html"
# 以下兩行擇一
response = requests.get(url, headers={"cookie":"over18=1"})  # 代替urlopen，也不能直接丟到BeautifulSoup
# 如果遇到403還是要加滿
# response = requests.get(url, headers={"cookie":"over18=1", "user-agent":"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36"})
response = requests.get(url, cookies={"over18":"1"})
html = BeautifulSoup(response.text)
print(html)

7. 找到要下載的圖片網址
import requests
from bs4 import BeautifulSoup

url = "https://www.ptt.cc/bbs/Beauty/M.1657591280.A.355.html"
response = requests.get(url, cookies={"over18":"1"})
html = BeautifulSoup(response.text)

allows = ["jpg", "jpeg", "png", "gif"]  # 被允許的副檔名
links = html.find_all("a")
for link in links:
    href = link["href"]
    sub = href.split(".")[-1]  # 取list中最後一個元素，也就是副檔名
    if sub.lower() in allows:  # 讓所有字母變成小寫
        print("download:", href)
    print("-" * 30)    

8. 下載圖片
import os
import requests
from bs4 import BeautifulSoup

url = "https://www.ptt.cc/bbs/Beauty/M.1657591280.A.355.html"
dn = "ptt/" + url.split("/")[-1]  # 資料夾檔名
if not os.path.exists(dn):  # 如果沒有這個資料夾就創建一個
    os.makedirs(dn)
    
response = requests.get(url, cookies={"over18":"1"})
html = BeautifulSoup(response.text)

allows = ["jpg", "jpeg", "png", "gif"]
links = html.find_all("a")
for link in links:
    href = link["href"]
    sub = href.split(".")[-1]
    if sub.lower() in allows:
        print("download:", href)
        # 如果是純文字直接用 response.text
        # 如果不是純文字(檔案)要帶入 stream=True 用 response.raw(檔案型態).read()
        response = requests.get(href, stream=True)  
        # 自己做存檔
        fp = dn + "/" + href.split("/")[-1]  # 做檔名跟副檔名
        # 純文字: open(檔名, "r"/"w", encoding="utf-8")
        # others: open(檔名, "rb"/"wb")
        f = open(fp, "wb")
        f.write(response.raw.read())  # 把裡面的內容讀出來寫入新檔案
        f.close()
    print("-" * 30)

#6 facebook(認識Get / POST)
登入錯的帳密(12345@gmail.com / abcd1234) -> F12 -> 第一個(?privacy開頭) -> Request URL -> 可以看到網址中少了帳號密碼
不是所有東西都可以放在網址上(帳號密碼)，要寫在一個秘密小紙條上，也可以發現Request Method: POST(正常會是GET)
GET: 沒有秘密(url、headers)
POST: 有秘密(url、headers、payload)
Payload -> Form Data -> encpass告訴你facebook真的沒有存你的帳號密碼

#7 CitiOrange公民報橘(https://buzzorange.com/citiorange/latest/)(換頁時最上面的網址不會換，像doodles)
點Load more news -> F12 -> General -> 可以看到網址中少了第幾頁，代表頁數被寫在秘密小紙條，並且Request Method: POST，驗證真的有秘密小紙條 -> 
Payload -> Form Data -> postOffset: 12(一頁12筆) -> securityCheck: 54f38af560(每天長得不一樣)

#8 Selenium
要有瀏覽器，所以不能使用線上環境 -> 有指令介面driver(跟python無關) -> 要有python library(Selenium)
1. 裝driver
先確認chrome版本 -> 三個點 -> 關於Chrome -> search:chrome driver -> 選第一個(ChromeDriver - WebDriver for Chrome) -> 根據自己的版本(Latest stable release: ChromeDriver 103.0.5060.53) -> 選擇自己電腦的型號

2. 開啟專案
開啟pycharm -> 把chromedriver拖曳到pythonProject資料夾

3. 安裝函式庫
win: file -> settings
mac: PyCharm -> Preferences -> Project:pythonProject -> Python Inerpreter(翻譯器) -> 按+ -> search: selenium -> Install Package

4. 引入driver
from selenium.webdriver import Chrome

driver = Chrome("./chromedriver")
driver.get("https://www.google.com")
driver.maximize_window()

5. 自動輸入
from selenium.webdriver.common.by import By
from selenium.webdriver import Chrome

driver = Chrome("./chromedriver")
driver.get("https://www.google.com")
driver.maximize_window()
# bs: find/find_all
# selenium: find_element/find_elements
element = driver.find_element(By.CLASS_NAME, "gLFyf").send_keys("浣熊")

6. 自動輸入(浣熊+ENTER) 
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver import Chrome

driver = Chrome("./chromedriver")
driver.get("https://www.google.com")
driver.maximize_window()
# bs: find/find_all
# selenium: find_element/find_elements
element = driver.find_element(By.CLASS_NAME, "gLFyf")
element.send_keys("浣熊")
element.send_keys(Keys.ENTER)

7. 抓連結(可以自製演唱會搶票程式)
F12 -> 方塊滑鼠點連結標題 -> 複製包住他的div class
import time
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.webdriver import Chrome

driver = Chrome("./chromedriver")
driver.get("https://www.google.com")
driver.maximize_window()
# bs: find/find_all
# selenium: find_element/find_elements
element = driver.find_element(By.CLASS_NAME, "gLFyf")
# 最常用 .send_keys("xxx")  .click()
element.send_keys("浣熊")
element.send_keys(Keys.ENTER)

time.sleep(1)  # 仿真一點，等一秒

elements = driver.find_elements(By.CLASS_NAME, "yuRUbf")
for e in elements:
    link = e.find_element(By.TAG_NAME, "a")
    # bs: .text/["href"]
    #selenium: .text/get_attribute("href")
    print(link.text)
    print(link.get_attribute("href"))
    print("-" * 30)

time.sleep(3)
driver.close()
